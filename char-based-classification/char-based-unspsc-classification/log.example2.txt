Using TensorFlow backend.
Loading data...
Creating vocab...
Chars vocab: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']
Chars vocab size: 27
X_train.shape: (64757, 100)
Build model...
Fit model...
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 100)               0
_________________________________________________________________
lambda_1 (Lambda)            (None, 100, 27)           0
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 100, 512)          97280
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 50, 512)           0
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 50, 512)           1835520
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 25, 512)           0
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 25, 512)           786944
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 25, 512)           786944
_________________________________________________________________
conv1d_5 (Conv1D)            (None, 23, 512)           786944
_________________________________________________________________
conv1d_6 (Conv1D)            (None, 21, 512)           786944
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 10, 512)           0
_________________________________________________________________
flatten_1 (Flatten)          (None, 5120)              0
_________________________________________________________________
dense_1 (Dense)              (None, 2048)              10487808
_________________________________________________________________
dropout_1 (Dropout)          (None, 2048)              0
_________________________________________________________________
dense_2 (Dense)              (None, 2048)              4196352
_________________________________________________________________
dropout_2 (Dropout)          (None, 2048)              0
_________________________________________________________________
output (Dense)               (None, 1007)              2063343
=================================================================
Total params: 21,828,079
Trainable params: 21,828,079
Non-trainable params: 0
_________________________________________________________________
Train on 64757 samples, validate on 27754 samples
Epoch 1/200
2018-03-09 02:44:58.671443: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 02:44:58.671459: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 02:44:58.671463: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 02:44:58.671467: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 02:44:58.671470: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 02:44:58.759337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-03-09 02:44:58.759626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 10.91GiB
Free memory: 10.08GiB
2018-03-09 02:44:58.759637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2018-03-09 02:44:58.759641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2018-03-09 02:44:58.759647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
64757/64757 [==============================] - 25s - loss: 3.5873 - acc: 0.3818 - val_loss: 2.3021 - val_acc: 0.5920
Epoch 2/200
64757/64757 [==============================] - 22s - loss: 1.9222 - acc: 0.6445 - val_loss: 1.4736 - val_acc: 0.7230
Epoch 3/200
64757/64757 [==============================] - 24s - loss: 1.3345 - acc: 0.7359 - val_loss: 1.1803 - val_acc: 0.7729
Epoch 4/200
64757/64757 [==============================] - 24s - loss: 1.0038 - acc: 0.7894 - val_loss: 0.9864 - val_acc: 0.8092
Epoch 5/200
64757/64757 [==============================] - 23s - loss: 0.7971 - acc: 0.8237 - val_loss: 0.9096 - val_acc: 0.8269
Epoch 6/200
64757/64757 [==============================] - 24s - loss: 0.6478 - acc: 0.8504 - val_loss: 0.8480 - val_acc: 0.8348
Epoch 7/200
64757/64757 [==============================] - 23s - loss: 0.5354 - acc: 0.8702 - val_loss: 0.8177 - val_acc: 0.8500
Epoch 8/200
64757/64757 [==============================] - 23s - loss: 0.4534 - acc: 0.8875 - val_loss: 0.8076 - val_acc: 0.8563
Epoch 9/200
64757/64757 [==============================] - 23s - loss: 0.3908 - acc: 0.9004 - val_loss: 0.7820 - val_acc: 0.8592
Epoch 10/200
64757/64757 [==============================] - 23s - loss: 0.3339 - acc: 0.9130 - val_loss: 0.7827 - val_acc: 0.8612
Epoch 11/200
64757/64757 [==============================] - 23s - loss: 0.3026 - acc: 0.9190 - val_loss: 0.7643 - val_acc: 0.8685
Epoch 12/200
64757/64757 [==============================] - 23s - loss: 0.2711 - acc: 0.9271 - val_loss: 0.8177 - val_acc: 0.8633
Epoch 13/200
64757/64757 [==============================] - 24s - loss: 0.2446 - acc: 0.9338 - val_loss: 0.7822 - val_acc: 0.8715
Epoch 14/200
64757/64757 [==============================] - 24s - loss: 0.2269 - acc: 0.9378 - val_loss: 0.8386 - val_acc: 0.8700
Epoch 15/200
64757/64757 [==============================] - 24s - loss: 0.2083 - acc: 0.9421 - val_loss: 0.8470 - val_acc: 0.8707
Epoch 16/200
64757/64757 [==============================] - 24s - loss: 0.1972 - acc: 0.9453 - val_loss: 0.8215 - val_acc: 0.8659
Epoch 17/200
64757/64757 [==============================] - 23s - loss: 0.1842 - acc: 0.9475 - val_loss: 0.8337 - val_acc: 0.8726
Epoch 18/200
64757/64757 [==============================] - 23s - loss: 0.1787 - acc: 0.9494 - val_loss: 0.8124 - val_acc: 0.8735
Epoch 19/200
64757/64757 [==============================] - 23s - loss: 0.1696 - acc: 0.9513 - val_loss: 0.8235 - val_acc: 0.8733
Epoch 20/200
64757/64757 [==============================] - 24s - loss: 0.1635 - acc: 0.9539 - val_loss: 0.8463 - val_acc: 0.8754
Epoch 21/200
64757/64757 [==============================] - 23s - loss: 0.1582 - acc: 0.9557 - val_loss: 0.8532 - val_acc: 0.8724
Epoch 22/200
64757/64757 [==============================] - 23s - loss: 0.1529 - acc: 0.9573 - val_loss: 0.8650 - val_acc: 0.8772
Epoch 23/200
64757/64757 [==============================] - 23s - loss: 0.1520 - acc: 0.9573 - val_loss: 0.8388 - val_acc: 0.8739
Epoch 24/200
64757/64757 [==============================] - 24s - loss: 0.1535 - acc: 0.9569 - val_loss: 0.8773 - val_acc: 0.8675
Epoch 25/200
64757/64757 [==============================] - 23s - loss: 0.1422 - acc: 0.9600 - val_loss: 0.8392 - val_acc: 0.8763
Epoch 26/200
64757/64757 [==============================] - 23s - loss: 0.1371 - acc: 0.9615 - val_loss: 0.8901 - val_acc: 0.8755
Epoch 27/200
64757/64757 [==============================] - 23s - loss: 0.1377 - acc: 0.9624 - val_loss: 0.8910 - val_acc: 0.8769
Epoch 28/200
64757/64757 [==============================] - 23s - loss: 0.1370 - acc: 0.9620 - val_loss: 0.8791 - val_acc: 0.8743
Epoch 29/200
64757/64757 [==============================] - 24s - loss: 0.1299 - acc: 0.9630 - val_loss: 0.8438 - val_acc: 0.8794
Epoch 30/200
64757/64757 [==============================] - 24s - loss: 0.1339 - acc: 0.9624 - val_loss: 0.8654 - val_acc: 0.8767
Epoch 31/200
64757/64757 [==============================] - 23s - loss: 0.1270 - acc: 0.9644 - val_loss: 0.8777 - val_acc: 0.8781
Epoch 32/200
64757/64757 [==============================] - 23s - loss: 0.1277 - acc: 0.9643 - val_loss: 0.8792 - val_acc: 0.8753
Epoch 33/200
64757/64757 [==============================] - 23s - loss: 0.1326 - acc: 0.9639 - val_loss: 0.8666 - val_acc: 0.8768
Epoch 34/200
64757/64757 [==============================] - 23s - loss: 0.1196 - acc: 0.9667 - val_loss: 0.8815 - val_acc: 0.8750
Epoch 35/200
64757/64757 [==============================] - 23s - loss: 0.1193 - acc: 0.9659 - val_loss: 0.9110 - val_acc: 0.8775
Epoch 36/200
64757/64757 [==============================] - 23s - loss: 0.1277 - acc: 0.9638 - val_loss: 0.8806 - val_acc: 0.8786
Epoch 37/200
64757/64757 [==============================] - 23s - loss: 0.1162 - acc: 0.9668 - val_loss: 0.8942 - val_acc: 0.8774
Epoch 38/200
64757/64757 [==============================] - 23s - loss: 0.1184 - acc: 0.9670 - val_loss: 0.8940 - val_acc: 0.8800
Epoch 39/200
64757/64757 [==============================] - 24s - loss: 0.1223 - acc: 0.9665 - val_loss: 0.9176 - val_acc: 0.8778
Epoch 40/200
64757/64757 [==============================] - 23s - loss: 0.1289 - acc: 0.9644 - val_loss: 0.9026 - val_acc: 0.8755
Epoch 41/200
64757/64757 [==============================] - 23s - loss: 0.1248 - acc: 0.9661 - val_loss: 0.8903 - val_acc: 0.8795
Epoch 42/200
64757/64757 [==============================] - 23s - loss: 0.1188 - acc: 0.9674 - val_loss: 0.9074 - val_acc: 0.8784
Epoch 43/200
64757/64757 [==============================] - 23s - loss: 0.1122 - acc: 0.9690 - val_loss: 0.9177 - val_acc: 0.8780
Epoch 44/200
64757/64757 [==============================] - 24s - loss: 0.1127 - acc: 0.9689 - val_loss: 0.8982 - val_acc: 0.8805
Epoch 45/200
64757/64757 [==============================] - 23s - loss: 0.1257 - acc: 0.9657 - val_loss: 0.9035 - val_acc: 0.8766
Epoch 46/200
64757/64757 [==============================] - 23s - loss: 0.1150 - acc: 0.9675 - val_loss: 0.8941 - val_acc: 0.8777
Epoch 47/200
64757/64757 [==============================] - 24s - loss: 0.1098 - acc: 0.9692 - val_loss: 0.9138 - val_acc: 0.8803
Epoch 48/200
64757/64757 [==============================] - 24s - loss: 0.1083 - acc: 0.9695 - val_loss: 0.8910 - val_acc: 0.8817
Epoch 49/200
64757/64757 [==============================] - 24s - loss: 0.1164 - acc: 0.9686 - val_loss: 0.8782 - val_acc: 0.8812
Epoch 50/200
64757/64757 [==============================] - 23s - loss: 0.1167 - acc: 0.9684 - val_loss: 0.9243 - val_acc: 0.8782
Epoch 51/200
64757/64757 [==============================] - 23s - loss: 0.1164 - acc: 0.9684 - val_loss: 0.9087 - val_acc: 0.8798
Epoch 52/200
64757/64757 [==============================] - 24s - loss: 0.1092 - acc: 0.9699 - val_loss: 0.8846 - val_acc: 0.8837
Epoch 53/200
64757/64757 [==============================] - 22s - loss: 0.1125 - acc: 0.9689 - val_loss: 0.9469 - val_acc: 0.8757
Epoch 54/200
64757/64757 [==============================] - 23s - loss: 0.1299 - acc: 0.9649 - val_loss: 0.9137 - val_acc: 0.8789
Epoch 55/200
64757/64757 [==============================] - 23s - loss: 0.1147 - acc: 0.9689 - val_loss: 0.9445 - val_acc: 0.8814
Epoch 56/200
64757/64757 [==============================] - 23s - loss: 0.1106 - acc: 0.9695 - val_loss: 0.9315 - val_acc: 0.8821
Epoch 57/200
64757/64757 [==============================] - 23s - loss: 0.1134 - acc: 0.9688 - val_loss: 0.9076 - val_acc: 0.8814
Epoch 58/200
64757/64757 [==============================] - 24s - loss: 0.1155 - acc: 0.9684 - val_loss: 0.9324 - val_acc: 0.8810
Epoch 59/200
64757/64757 [==============================] - 23s - loss: 0.1178 - acc: 0.9688 - val_loss: 0.9462 - val_acc: 0.8802
Epoch 60/200
64757/64757 [==============================] - 23s - loss: 0.1078 - acc: 0.9706 - val_loss: 0.8979 - val_acc: 0.8834
Epoch 61/200
64757/64757 [==============================] - 23s - loss: 0.1117 - acc: 0.9702 - val_loss: 0.9489 - val_acc: 0.8794
Epoch 62/200
64757/64757 [==============================] - 24s - loss: 0.1238 - acc: 0.9671 - val_loss: 0.8969 - val_acc: 0.8793
Epoch 63/200
64757/64757 [==============================] - 23s - loss: 0.1109 - acc: 0.9698 - val_loss: 0.9293 - val_acc: 0.8817
Epoch 64/200
64757/64757 [==============================] - 23s - loss: 0.1066 - acc: 0.9710 - val_loss: 0.9180 - val_acc: 0.8815
Epoch 65/200
64757/64757 [==============================] - 24s - loss: 0.1104 - acc: 0.9704 - val_loss: 0.9412 - val_acc: 0.8816
Epoch 66/200
64757/64757 [==============================] - 23s - loss: 0.1183 - acc: 0.9695 - val_loss: 0.9127 - val_acc: 0.8809
Epoch 67/200
64757/64757 [==============================] - 23s - loss: 0.1187 - acc: 0.9686 - val_loss: 0.9426 - val_acc: 0.8781
Epoch 68/200
64757/64757 [==============================] - 23s - loss: 0.1261 - acc: 0.9680 - val_loss: 0.9361 - val_acc: 0.8809
Epoch 69/200
64757/64757 [==============================] - 23s - loss: 0.1171 - acc: 0.9692 - val_loss: 0.9458 - val_acc: 0.8798
Epoch 70/200
64757/64757 [==============================] - 23s - loss: 0.1050 - acc: 0.9710 - val_loss: 0.9043 - val_acc: 0.8816
Epoch 71/200
64757/64757 [==============================] - 23s - loss: 0.1105 - acc: 0.9699 - val_loss: 0.9528 - val_acc: 0.8818
Epoch 72/200
64757/64757 [==============================] - 23s - loss: 0.1155 - acc: 0.9694 - val_loss: 0.9378 - val_acc: 0.8819
Epoch 73/200
64757/64757 [==============================] - 24s - loss: 0.1169 - acc: 0.9689 - val_loss: 0.9166 - val_acc: 0.8819
Epoch 74/200
64757/64757 [==============================] - 23s - loss: 0.1152 - acc: 0.9701 - val_loss: 0.9700 - val_acc: 0.8819
Epoch 75/200
64757/64757 [==============================] - 23s - loss: 0.1170 - acc: 0.9699 - val_loss: 0.9552 - val_acc: 0.8803
Epoch 76/200
64757/64757 [==============================] - 23s - loss: 0.1164 - acc: 0.9692 - val_loss: 0.9472 - val_acc: 0.8793
Epoch 77/200
64757/64757 [==============================] - 23s - loss: 0.1219 - acc: 0.9682 - val_loss: 0.9414 - val_acc: 0.8813
Epoch 78/200
64757/64757 [==============================] - 23s - loss: 0.1219 - acc: 0.9680 - val_loss: 0.9272 - val_acc: 0.8813
