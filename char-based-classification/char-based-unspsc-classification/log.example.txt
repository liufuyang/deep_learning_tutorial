Using TensorFlow backend.
Loading data...
Creating vocab...
Chars vocab: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '!', '"', '#', '$', '%', '&', "'", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~', '\n', ' ']
Chars vocab size: 70
X_train.shape: (64757, 100)
Build model...
Fit model...
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 100)               0
_________________________________________________________________
lambda_1 (Lambda)            (None, 100, 70)           0
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 100, 256)          125696
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 50, 256)           0
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 50, 512)           655872
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 25, 512)           0
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 25, 512)           786944
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 25, 1024)          1049600
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 12, 1024)          0
_________________________________________________________________
flatten_1 (Flatten)          (None, 12288)             0
_________________________________________________________________
dense_1 (Dense)              (None, 2048)              25167872
_________________________________________________________________
dropout_1 (Dropout)          (None, 2048)              0
_________________________________________________________________
output (Dense)               (None, 1007)              2063343
=================================================================
Total params: 29,849,327
Trainable params: 29,849,327
Non-trainable params: 0
_________________________________________________________________
Train on 64757 samples, validate on 27754 samples
Epoch 1/200
2018-01-22 00:19:38.791520: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-22 00:19:38.791537: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-22 00:19:38.791541: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-22 00:19:38.791544: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-22 00:19:38.791548: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-01-22 00:19:38.903460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-22 00:19:38.903753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 10.91GiB
Free memory: 10.06GiB
2018-01-22 00:19:38.903765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2018-01-22 00:19:38.903769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2018-01-22 00:19:38.903774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
64757/64757 [==============================] - 29s - loss: 4.1858 - acc: 0.2478 - val_loss: 3.2393 - val_acc: 0.3996
Epoch 2/200
64757/64757 [==============================] - 27s - loss: 2.8516 - acc: 0.4705 - val_loss: 2.4492 - val_acc: 0.5487
Epoch 3/200
64757/64757 [==============================] - 27s - loss: 2.2564 - acc: 0.5737 - val_loss: 2.0710 - val_acc: 0.6167
Epoch 4/200
64757/64757 [==============================] - 28s - loss: 1.8988 - acc: 0.6330 - val_loss: 1.7966 - val_acc: 0.6671
Epoch 5/200
64757/64757 [==============================] - 27s - loss: 1.6368 - acc: 0.6779 - val_loss: 1.6348 - val_acc: 0.7042
Epoch 6/200
64757/64757 [==============================] - 27s - loss: 1.4402 - acc: 0.7118 - val_loss: 1.5088 - val_acc: 0.7256
Epoch 7/200
64757/64757 [==============================] - 27s - loss: 1.2792 - acc: 0.7376 - val_loss: 1.4613 - val_acc: 0.7401
Epoch 8/200
64757/64757 [==============================] - 27s - loss: 1.1481 - acc: 0.7600 - val_loss: 1.3697 - val_acc: 0.7590
Epoch 9/200
64757/64757 [==============================] - 27s - loss: 1.0378 - acc: 0.7783 - val_loss: 1.3376 - val_acc: 0.7671
Epoch 10/200
64757/64757 [==============================] - 27s - loss: 0.9460 - acc: 0.7953 - val_loss: 1.2830 - val_acc: 0.7767
Epoch 11/200
64757/64757 [==============================] - 27s - loss: 0.8583 - acc: 0.8111 - val_loss: 1.2390 - val_acc: 0.7867
Epoch 12/200
64757/64757 [==============================] - 28s - loss: 0.7884 - acc: 0.8210 - val_loss: 1.2370 - val_acc: 0.7949
Epoch 13/200
64757/64757 [==============================] - 27s - loss: 0.7304 - acc: 0.8332 - val_loss: 1.2784 - val_acc: 0.7943
Epoch 14/200
64757/64757 [==============================] - 27s - loss: 0.6649 - acc: 0.8461 - val_loss: 1.2098 - val_acc: 0.8042
Epoch 15/200
64757/64757 [==============================] - 27s - loss: 0.6183 - acc: 0.8544 - val_loss: 1.2002 - val_acc: 0.8122
Epoch 16/200
64757/64757 [==============================] - 27s - loss: 0.5744 - acc: 0.8631 - val_loss: 1.2403 - val_acc: 0.8104
Epoch 17/200
64757/64757 [==============================] - 27s - loss: 0.5323 - acc: 0.8724 - val_loss: 1.2141 - val_acc: 0.8130
Epoch 18/200
64757/64757 [==============================] - 27s - loss: 0.5010 - acc: 0.8784 - val_loss: 1.2259 - val_acc: 0.8140
Epoch 19/200
64757/64757 [==============================] - 27s - loss: 0.4644 - acc: 0.8865 - val_loss: 1.2495 - val_acc: 0.8209
Epoch 20/200
64757/64757 [==============================] - 27s - loss: 0.4264 - acc: 0.8943 - val_loss: 1.2311 - val_acc: 0.8242
Epoch 21/200
64757/64757 [==============================] - 27s - loss: 0.4047 - acc: 0.8995 - val_loss: 1.2356 - val_acc: 0.8207
Epoch 22/200
64757/64757 [==============================] - 27s - loss: 0.3774 - acc: 0.9047 - val_loss: 1.2157 - val_acc: 0.8259
Epoch 23/200
64757/64757 [==============================] - 27s - loss: 0.3596 - acc: 0.9100 - val_loss: 1.2180 - val_acc: 0.8222
Epoch 24/200
64757/64757 [==============================] - 27s - loss: 0.3470 - acc: 0.9110 - val_loss: 1.2429 - val_acc: 0.8307
Epoch 25/200
64757/64757 [==============================] - 27s - loss: 0.3182 - acc: 0.9181 - val_loss: 1.2286 - val_acc: 0.8333
Epoch 26/200
64757/64757 [==============================] - 27s - loss: 0.3010 - acc: 0.9226 - val_loss: 1.2360 - val_acc: 0.8328
Epoch 27/200
64757/64757 [==============================] - 27s - loss: 0.2910 - acc: 0.9259 - val_loss: 1.2529 - val_acc: 0.8308
Epoch 28/200
64757/64757 [==============================] - 27s - loss: 0.2679 - acc: 0.9309 - val_loss: 1.2548 - val_acc: 0.8356
Epoch 29/200
64757/64757 [==============================] - 28s - loss: 0.2620 - acc: 0.9324 - val_loss: 1.2538 - val_acc: 0.8312
Epoch 30/200
64757/64757 [==============================] - 27s - loss: 0.2463 - acc: 0.9355 - val_loss: 1.3273 - val_acc: 0.8377
Epoch 31/200
64757/64757 [==============================] - 27s - loss: 0.2402 - acc: 0.9370 - val_loss: 1.2557 - val_acc: 0.8371
Epoch 32/200
64757/64757 [==============================] - 27s - loss: 0.2259 - acc: 0.9399 - val_loss: 1.2921 - val_acc: 0.8378
Epoch 33/200
64757/64757 [==============================] - 27s - loss: 0.2157 - acc: 0.9437 - val_loss: 1.3065 - val_acc: 0.8392
Epoch 34/200
64757/64757 [==============================] - 27s - loss: 0.2037 - acc: 0.9467 - val_loss: 1.2816 - val_acc: 0.8384
Epoch 35/200
64757/64757 [==============================] - 27s - loss: 0.2008 - acc: 0.9472 - val_loss: 1.2556 - val_acc: 0.8456
Epoch 36/200
64757/64757 [==============================] - 27s - loss: 0.1840 - acc: 0.9515 - val_loss: 1.3087 - val_acc: 0.8434
Epoch 37/200
64757/64757 [==============================] - 27s - loss: 0.1809 - acc: 0.9532 - val_loss: 1.3088 - val_acc: 0.8402
Epoch 38/200
64757/64757 [==============================] - 27s - loss: 0.1765 - acc: 0.9539 - val_loss: 1.3307 - val_acc: 0.8435
Epoch 39/200
64757/64757 [==============================] - 27s - loss: 0.1685 - acc: 0.9548 - val_loss: 1.3735 - val_acc: 0.8430
Epoch 40/200
64757/64757 [==============================] - 27s - loss: 0.1613 - acc: 0.9579 - val_loss: 1.3580 - val_acc: 0.8462
Epoch 41/200
64757/64757 [==============================] - 27s - loss: 0.1546 - acc: 0.9592 - val_loss: 1.3254 - val_acc: 0.8446
Epoch 42/200
64757/64757 [==============================] - 27s - loss: 0.1503 - acc: 0.9609 - val_loss: 1.3351 - val_acc: 0.8454
Epoch 43/200
64757/64757 [==============================] - 27s - loss: 0.1508 - acc: 0.9604 - val_loss: 1.3592 - val_acc: 0.8431
Epoch 44/200
64757/64757 [==============================] - 27s - loss: 0.1459 - acc: 0.9616 - val_loss: 1.3185 - val_acc: 0.8488
Epoch 45/200
64757/64757 [==============================] - 28s - loss: 0.1331 - acc: 0.9646 - val_loss: 1.3668 - val_acc: 0.8476
Epoch 46/200
64757/64757 [==============================] - 27s - loss: 0.1341 - acc: 0.9648 - val_loss: 1.3494 - val_acc: 0.8482
Epoch 47/200
64757/64757 [==============================] - 27s - loss: 0.1339 - acc: 0.9644 - val_loss: 1.3486 - val_acc: 0.8469
Epoch 48/200
64757/64757 [==============================] - 27s - loss: 0.1258 - acc: 0.9667 - val_loss: 1.3605 - val_acc: 0.8485
Epoch 49/200
64757/64757 [==============================] - 27s - loss: 0.1232 - acc: 0.9681 - val_loss: 1.3479 - val_acc: 0.8495
Epoch 50/200
64757/64757 [==============================] - 27s - loss: 0.1190 - acc: 0.9690 - val_loss: 1.3892 - val_acc: 0.8495
