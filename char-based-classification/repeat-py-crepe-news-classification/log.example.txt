Using TensorFlow backend.
Loading data...
Creating vocab...
Build model...
Chars vocab: {'w', '?', '{', '|', '&', 'o', 'm', '2', '4', 'a', 'u', 'b', ')', '_', 'i', 'k', '[', 'y', 'e', 'n', 'h', '}', '\n', 'p', 't', '3', '>', '*', 'z', ']', '5', '/', '-', '~', '6', 'j', 'l', 'q', 'r', 'v', '#', '\\', 'd', '.', '$', ',', '9', ':', '`', '%', '(', '=', 'c', '7', 'x', "'", 'f', 'g', '1', '+', ';', '0', '<', '^', '"', '!', 's', '8', '@'}
Chars vocab size: 69
X_train.shape: (120000, 1014)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1014)              0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 1014, 69)          0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 1008, 256)         123904    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 336, 256)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 330, 256)          459008    
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 110, 256)          0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 108, 256)          196864    
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 106, 256)          196864    
_________________________________________________________________
conv1d_5 (Conv1D)            (None, 104, 256)          196864    
_________________________________________________________________
conv1d_6 (Conv1D)            (None, 102, 256)          196864    
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 34, 256)           0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 8704)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 1024)              8913920   
_________________________________________________________________
dropout_1 (Dropout)          (None, 1024)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 1024)              1049600   
_________________________________________________________________
dropout_2 (Dropout)          (None, 1024)              0         
_________________________________________________________________
output (Dense)               (None, 4)                 4100      
=================================================================
Total params: 11,337,988
Trainable params: 11,337,988
Non-trainable params: 0
_________________________________________________________________
Fit model...
Train on 120000 samples, validate on 7600 samples
Epoch 1/20
2018-03-18 17:51:32.515700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 10.23GiB
2018-03-18 17:51:32.515712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-03-18 17:51:32.680458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9901 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
120000/120000 [==============================] - 59s 494us/step - loss: 0.9338 - acc: 0.5690 - val_loss: 0.4444 - val_acc: 0.8392
Epoch 2/20
120000/120000 [==============================] - 61s 506us/step - loss: 0.3869 - acc: 0.8646 - val_loss: 0.3884 - val_acc: 0.8662
Epoch 3/20
120000/120000 [==============================] - 61s 505us/step - loss: 0.3017 - acc: 0.8973 - val_loss: 0.3604 - val_acc: 0.8772
Epoch 4/20
120000/120000 [==============================] - 60s 503us/step - loss: 0.2582 - acc: 0.9125 - val_loss: 0.3286 - val_acc: 0.8876
Epoch 5/20
120000/120000 [==============================] - 60s 502us/step - loss: 0.2245 - acc: 0.9244 - val_loss: 0.3785 - val_acc: 0.8671
Epoch 6/20
120000/120000 [==============================] - 60s 499us/step - loss: 0.1955 - acc: 0.9334 - val_loss: 0.3502 - val_acc: 0.8913
Epoch 7/20
120000/120000 [==============================] - 60s 504us/step - loss: 0.1670 - acc: 0.9443 - val_loss: 0.3765 - val_acc: 0.8813
Epoch 8/20
120000/120000 [==============================] - 60s 504us/step - loss: 0.1446 - acc: 0.9511 - val_loss: 0.4315 - val_acc: 0.8818
Epoch 9/20
120000/120000 [==============================] - 61s 504us/step - loss: 0.1253 - acc: 0.9575 - val_loss: 0.4373 - val_acc: 0.8762
Epoch 10/20
120000/120000 [==============================] - 60s 503us/step - loss: 0.1122 - acc: 0.9619 - val_loss: 0.4629 - val_acc: 0.8736
